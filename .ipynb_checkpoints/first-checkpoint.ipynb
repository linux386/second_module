{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "from tqdm import tqdm\n",
    "sns.set()\n",
    "tf.compat.v1.random.set_random_seed(1234)\n",
    "\n",
    "f =  open('d:\\\\/stock_prediction with LSTM_version-1.txt', 'a')\n",
    "print(\"===== Stock Prediction with LSTM_first =====\", file = f)\n",
    "print(\"start_day: {}\\n\".format(datetime.now()),file = f)\n",
    "\n",
    "tf.set_random_seed(777)  # reproducibility\n",
    "tf.reset_default_graph()\n",
    "\n",
    "test_min = np.min(xy, 0)\n",
    "test_max = np.max(xy, 0)\n",
    "test_denom = test_max - test_min\n",
    "\n",
    "engine = sqlalchemy.create_engine('mysql+pymysql://kkang:leaf2027@localhost/stock?charset=utf8',encoding='utf-8')\n",
    "    \n",
    "name = input('주식이름을 입력하세요 : ')\n",
    "date = input(\"날짜를 입력하세요 sample: '2019-01-10':\")\n",
    "        \n",
    "select_query = \"select * from market where Name= \"\n",
    "date_query = \"Date > \"\n",
    "\n",
    "var = select_query +\"'\"+name+\"'\"+\" \"+\"&&\"+\" \"+date_query+\"'\"+date+\"'\"\n",
    "\n",
    "df = pd.read_sql(var ,engine)\n",
    "#df = df[['Open','High','Low','Volume','Close']]\n",
    "df = df.iloc[:,3:]\n",
    "xy = df.values\n",
    "\n",
    "seq_length = 5\n",
    "data_dim = 5\n",
    "hidden_dim = 10\n",
    "output_dim = 1\n",
    "learning_rate = 0.01\n",
    "iterations = 900\n",
    "\n",
    "minmax = MinMaxScaler().fit(df.astype('float64')) # Close index\n",
    "dataset = minmax.transform(df.astype('float64')) # Close index\n",
    "\n",
    "train_size = int(len(dataset) * 0.7)\n",
    "train_set = dataset[0:train_size]\n",
    "test_set = dataset[train_size-seq_length+1:]  # Index from [train_size - seq_leng\n",
    "\n",
    "# build datasets\n",
    "def build_dataset(time_series, seq_length):\n",
    "    dataX = []\n",
    "    dataY = []\n",
    "    for i in range(0, len(time_series) - seq_length):\n",
    "        _x = time_series[i:i + seq_length, :]\n",
    "        _y = time_series[i + seq_length, [-1]]  # Next close price\n",
    "        #print(_x, \"->\", _y)\n",
    "        dataX.append(_x)\n",
    "        dataY.append(_y)\n",
    "    return np.array(dataX), np.array(dataY)\n",
    "\n",
    "trainX, trainY = build_dataset(train_set, seq_length)\n",
    "#print(\"#\"*100)\n",
    "testX, testY = build_dataset(test_set, seq_length)\n",
    "#print(\"#\"*100)\n",
    "#lastX = last_set\n",
    "#print(lastX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "from tqdm import tqdm\n",
    "sns.set()\n",
    "tf.compat.v1.random.set_random_seed(1234)\n",
    "\n",
    "f =  open('d:\\\\/stock_prediction with LSTM_version-1.txt', 'a')\n",
    "print(\"===== Stock Prediction with LSTM_first =====\", file = f)\n",
    "print(\"start_day: {}\\n\".format(datetime.now()),file = f)\n",
    "\n",
    "tf.set_random_seed(777)  # reproducibility\n",
    "tf.reset_default_graph()\n",
    "\n",
    "test_min = np.min(xy, 0)\n",
    "test_max = np.max(xy, 0)\n",
    "test_denom = test_max - test_min\n",
    "\n",
    "engine = sqlalchemy.create_engine('mysql+pymysql://kkang:leaf2027@localhost/stock?charset=utf8',encoding='utf-8')\n",
    "    \n",
    "name = input('주식이름을 입력하세요 : ')\n",
    "date = input(\"날짜를 입력하세요 sample: '2019-01-10':\")\n",
    "        \n",
    "select_query = \"select * from market where Name= \"\n",
    "date_query = \"Date > \"\n",
    "\n",
    "var = select_query +\"'\"+name+\"'\"+\" \"+\"&&\"+\" \"+date_query+\"'\"+date+\"'\"\n",
    "\n",
    "df = pd.read_sql(var ,engine)\n",
    "#df = df[['Open','High','Low','Volume','Close']]\n",
    "df = df.iloc[:,3:]\n",
    "xy = df.values\n",
    "\n",
    "seq_length = 5\n",
    "data_dim = 5\n",
    "hidden_dim = 10\n",
    "output_dim = 1\n",
    "learning_rate = 0.01\n",
    "iterations = 900\n",
    "\n",
    "minmax = MinMaxScaler().fit(df.astype('float64')) # Close index\n",
    "dataset = minmax.transform(df.astype('float64')) # Close index\n",
    "\n",
    "train_size = int(len(dataset) * 0.7)\n",
    "train_set = dataset[0:train_size]\n",
    "test_set = dataset[train_size-seq_length+1:]  # Index from [train_size - seq_leng\n",
    "\n",
    "# build datasets\n",
    "def build_dataset(time_series, seq_length):\n",
    "    dataX = []\n",
    "    dataY = []\n",
    "    for i in range(0, len(time_series) - seq_length):\n",
    "        _x = time_series[i:i + seq_length, :]\n",
    "        _y = time_series[i + seq_length, [-1]]  # Next close price\n",
    "        #print(_x, \"->\", _y)\n",
    "        dataX.append(_x)\n",
    "        dataY.append(_y)\n",
    "    return np.array(dataX), np.array(dataY)\n",
    "\n",
    "trainX, trainY = build_dataset(train_set, seq_length)\n",
    "#print(\"#\"*100)\n",
    "testX, testY = build_dataset(test_set, seq_length)\n",
    "#print(\"#\"*100)\n",
    "#lastX = last_set\n",
    "#print(lastX)\n",
    "\n",
    "# input place holders\n",
    "X = tf.placeholder(tf.float32, [None, seq_length, data_dim])\n",
    "Y = tf.placeholder(tf.float32, [None, 1])\n",
    "\n",
    "# build a LSTM network\n",
    "cell = tf.nn.rnn_cell.LSTMCell(num_units=hidden_dim, state_is_tuple=True, activation=tf.tanh)\n",
    "outputs, _states = tf.nn.dynamic_rnn(cell, X, dtype=tf.float32)\n",
    "Y_pred = tf.contrib.layers.fully_connected(outputs[:, -1], output_dim, activation_fn=None)  # We use the last cell's output\n",
    "\n",
    "# cost/loss\n",
    "loss = tf.reduce_sum(tf.square(Y_pred - Y))  # sum of the squares\n",
    "# optimizer\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "train = optimizer.minimize(loss)\n",
    "\n",
    "# RMSE\n",
    "targets = tf.placeholder(tf.float32, [None, 1])\n",
    "predictions = tf.placeholder(tf.float32, [None, 1])\n",
    "rmse = tf.sqrt(tf.reduce_mean(tf.square(targets - predictions)))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "\n",
    "    # Training step\n",
    "    for i in range(iterations):\n",
    "        _, step_loss = sess.run([train, loss], feed_dict={X: trainX, Y: trainY})\n",
    "        if i % 100 == 0:\n",
    "            print(\"[step: {}] loss: {}\".format(i, step_loss))\n",
    "\n",
    "    # Test step\n",
    "    test_predict = sess.run(Y_pred, feed_dict={X: testX})\n",
    "    print(\"test_predict:{}\".format(test_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MinMaxScaler.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "from tqdm import tqdm\n",
    "sns.set()\n",
    "tf.compat.v1.random.set_random_seed(1234)\n",
    "\n",
    "f =  open('d:\\\\/stock_prediction with LSTM_version-1.txt', 'a')\n",
    "print(\"===== Stock Prediction with LSTM_first =====\", file = f)\n",
    "print(\"start_day: {}\\n\".format(datetime.now()),file = f)\n",
    "\n",
    "tf.set_random_seed(777)  # reproducibility\n",
    "tf.reset_default_graph()\n",
    "\n",
    "test_min = np.min(xy, 0)\n",
    "test_max = np.max(xy, 0)\n",
    "test_denom = test_max - test_min\n",
    "\n",
    "engine = sqlalchemy.create_engine('mysql+pymysql://kkang:leaf2027@localhost/stock?charset=utf8',encoding='utf-8')\n",
    "    \n",
    "name = input('주식이름을 입력하세요 : ')\n",
    "date = input(\"날짜를 입력하세요 sample: '2019-01-10':\")\n",
    "        \n",
    "select_query = \"select * from market where Name= \"\n",
    "date_query = \"Date > \"\n",
    "\n",
    "var = select_query +\"'\"+name+\"'\"+\" \"+\"&&\"+\" \"+date_query+\"'\"+date+\"'\"\n",
    "\n",
    "df = pd.read_sql(var ,engine)\n",
    "#df = df[['Open','High','Low','Volume','Close']]\n",
    "df = df.iloc[:,3:]\n",
    "xy = df.values\n",
    "\n",
    "seq_length = 5\n",
    "data_dim = 5\n",
    "hidden_dim = 10\n",
    "output_dim = 1\n",
    "learning_rate = 0.01\n",
    "iterations = 900\n",
    "\n",
    "minmax = MinMaxScaler().fit(df.astype('float64')) # Close index\n",
    "dataset = minmax.transform(df.astype('float64')) # Close index\n",
    "\n",
    "train_size = int(len(dataset) * 0.7)\n",
    "train_set = dataset[0:train_size]\n",
    "test_set = dataset[train_size-seq_length+1:]  # Index from [train_size - seq_leng\n",
    "\n",
    "# build datasets\n",
    "def build_dataset(time_series, seq_length):\n",
    "    dataX = []\n",
    "    dataY = []\n",
    "    for i in range(0, len(time_series) - seq_length):\n",
    "        _x = time_series[i:i + seq_length, :]\n",
    "        _y = time_series[i + seq_length, [-1]]  # Next close price\n",
    "        #print(_x, \"->\", _y)\n",
    "        dataX.append(_x)\n",
    "        dataY.append(_y)\n",
    "    return np.array(dataX), np.array(dataY)\n",
    "\n",
    "trainX, trainY = build_dataset(train_set, seq_length)\n",
    "#print(\"#\"*100)\n",
    "testX, testY = build_dataset(test_set, seq_length)\n",
    "#print(\"#\"*100)\n",
    "#lastX = last_set\n",
    "#print(lastX)\n",
    "\n",
    "# input place holders\n",
    "X = tf.placeholder(tf.float32, [None, seq_length, data_dim])\n",
    "Y = tf.placeholder(tf.float32, [None, 1])\n",
    "\n",
    "# build a LSTM network\n",
    "cell = tf.nn.rnn_cell.LSTMCell(num_units=hidden_dim, state_is_tuple=True, activation=tf.tanh)\n",
    "outputs, _states = tf.nn.dynamic_rnn(cell, X, dtype=tf.float32)\n",
    "Y_pred = tf.contrib.layers.fully_connected(outputs[:, -1], output_dim, activation_fn=None)  # We use the last cell's output\n",
    "\n",
    "# cost/loss\n",
    "loss = tf.reduce_sum(tf.square(Y_pred - Y))  # sum of the squares\n",
    "# optimizer\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "train = optimizer.minimize(loss)\n",
    "\n",
    "# RMSE\n",
    "targets = tf.placeholder(tf.float32, [None, 1])\n",
    "predictions = tf.placeholder(tf.float32, [None, 1])\n",
    "rmse = tf.sqrt(tf.reduce_mean(tf.square(targets - predictions)))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "\n",
    "    # Training step\n",
    "    for i in range(iterations):\n",
    "        _, step_loss = sess.run([train, loss], feed_dict={X: trainX, Y: trainY})\n",
    "        if i % 100 == 0:\n",
    "            print(\"[step: {}] loss: {}\".format(i, step_loss))\n",
    "\n",
    "    # Test step\n",
    "    test_predict = sess.run(Y_pred, feed_dict={X: testX})\n",
    "    print(\"test_predict:{}\".format(test_predict))\n",
    "    rmse_val = sess.run(rmse, feed_dict={targets: testY, predictions: test_predict})\n",
    "    \n",
    "    \n",
    "    # Predictions test\n",
    "    #prediction_last = sess.run(Y_pred, feed_dict={X: lastX.reshape(1,seq_length,data_dim)})\n",
    "\n",
    "print(\"step_loss: {}\".format(step_loss),file = f)\n",
    "print(\"RMSE: {}\\n\".format(rmse_val),file = f)\n",
    "print(\"step_loss: {}\".format(step_loss))\n",
    "print(\"predictions \", end='')\n",
    "print(\"RMSE: {}\\n\".format(rmse_val))\n",
    "#print(\"prediction_last :{}\". format(prediction_last) )\n",
    "df1 = ((testY * ((test_max - test_min) + 1e-7) + test_min).astype(int))\n",
    "df1 = pd.DataFrame(df1)\n",
    "testY = df1[4]\n",
    "df2 = ((test_predict * ((test_max - test_min) + 1e-7) + test_min).astype(int))\n",
    "df2= pd.DataFrame(df2)\n",
    "pred = df2[4]\n",
    "df = pd.concat([pred,testY], axis=1)\n",
    "df.columns=['Pred','True']\n",
    "display(df)\n",
    "df['Pred'].plot(figsize=(16,4))\n",
    "df['True'].plot()\n",
    "plt.legend(loc=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MinMaxScaler(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### MinMaxScaler(data)\n",
    "\n",
    "from __future__ import print_function\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sqlalchemy\n",
    "from datetime import datetime\n",
    "import tensorflow as tf\n",
    "\n",
    "f =  open('d:\\\\/stock_prediction with LSTM_version-1.txt', 'a')\n",
    "print(\"===== Stock Prediction with LSTM_first =====\", file = f)\n",
    "print(\"start_day: {}\\n\".format(datetime.now()),file = f)\n",
    "\n",
    "tf.set_random_seed(777)  # reproducibility\n",
    "tf.reset_default_graph()\n",
    "\n",
    "def MinMaxScaler(data):\n",
    "    numerator = data - np.min(data, 0)\n",
    "    denominator = np.max(data, 0) - np.min(data, 0)\n",
    "    # noise term prevents the zero division\n",
    "    return numerator / (denominator + 1e-7)\n",
    "\n",
    "engine = sqlalchemy.create_engine('mysql+pymysql://kkang:leaf2027@localhost/stock?charset=utf8',encoding='utf-8')\n",
    "    \n",
    "name = input('주식이름을 입력하세요 : ')\n",
    "date = input(\"날짜를 입력하세요 sample: '2019-01-10':\")\n",
    "        \n",
    "select_query = \"select * from market where Name= \"\n",
    "date_query = \"Date > \"\n",
    "\n",
    "var = select_query +\"'\"+name+\"'\"+\" \"+\"&&\"+\" \"+date_query+\"'\"+date+\"'\"\n",
    "\n",
    "df = pd.read_sql(var ,engine)\n",
    "#df = df[['Open','High','Low','Volume','Close']]\n",
    "df = df.iloc[:,3:]\n",
    "xy = df.values\n",
    "\n",
    "seq_length = 5\n",
    "data_dim = 5\n",
    "hidden_dim = 10\n",
    "output_dim = 1\n",
    "learning_rate = 0.01\n",
    "iterations = 900\n",
    "\n",
    "test_min = np.min(xy, 0)\n",
    "test_max = np.max(xy, 0)\n",
    "test_denom = test_max - test_min\n",
    "\n",
    "dataset = MinMaxScaler(xy)\n",
    "\n",
    "train_size = int(len(xy) * 0.7)\n",
    "train_set = dataset[0:train_size]\n",
    "test_set = dataset[train_size-seq_length+1:]  # Index from [train_size - seq_leng\n",
    "\n",
    "# build datasets\n",
    "def build_dataset(time_series, seq_length):\n",
    "    dataX = []\n",
    "    dataY = []\n",
    "    for i in range(0, len(time_series) - seq_length):\n",
    "        _x = time_series[i:i + seq_length, :]\n",
    "        _y = time_series[i + seq_length, [-1]]  # Next close price\n",
    "        #print(_x, \"->\", _y)\n",
    "        dataX.append(_x)\n",
    "        dataY.append(_y)\n",
    "    return np.array(dataX), np.array(dataY)\n",
    "\n",
    "trainX, trainY = build_dataset(train_set, seq_length)\n",
    "#print(\"#\"*100)\n",
    "testX, testY = build_dataset(test_set, seq_length)\n",
    "#print(\"#\"*100)\n",
    "#lastX = last_set\n",
    "#print(lastX)\n",
    "\n",
    "# input place holders\n",
    "X = tf.placeholder(tf.float32, [None, seq_length, data_dim])\n",
    "Y = tf.placeholder(tf.float32, [None, 1])\n",
    "\n",
    "# build a LSTM network\n",
    "cell = tf.nn.rnn_cell.LSTMCell(num_units=hidden_dim, state_is_tuple=True, activation=tf.tanh)\n",
    "outputs, _states = tf.nn.dynamic_rnn(cell, X, dtype=tf.float32)\n",
    "Y_pred = tf.contrib.layers.fully_connected(outputs[:, -1], output_dim, activation_fn=None)  # We use the last cell's output\n",
    "\n",
    "# cost/loss\n",
    "loss = tf.reduce_sum(tf.square(Y_pred - Y))  # sum of the squares\n",
    "# optimizer\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "train = optimizer.minimize(loss)\n",
    "\n",
    "# RMSE\n",
    "targets = tf.placeholder(tf.float32, [None, 1])\n",
    "predictions = tf.placeholder(tf.float32, [None, 1])\n",
    "rmse = tf.sqrt(tf.reduce_mean(tf.square(targets - predictions)))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "\n",
    "    # Training step\n",
    "    for i in range(iterations):\n",
    "        _, step_loss = sess.run([train, loss], feed_dict={X: trainX, Y: trainY})\n",
    "        if i % 100 == 0:\n",
    "            print(\"[step: {}] loss: {}\".format(i, step_loss))\n",
    "\n",
    "    # Test step\n",
    "    test_predict = sess.run(Y_pred, feed_dict={X: testX})\n",
    "    print(\"test_predict:{}\".format(test_predict))\n",
    "    rmse_val = sess.run(rmse, feed_dict={targets: testY, predictions: test_predict})\n",
    "    \n",
    "    \n",
    "    # Predictions test\n",
    "    #prediction_last = sess.run(Y_pred, feed_dict={X: lastX.reshape(1,seq_length,data_dim)})\n",
    "\n",
    "print(\"step_loss: {}\".format(step_loss),file = f)\n",
    "print(\"RMSE: {}\\n\".format(rmse_val),file = f)\n",
    "print(\"step_loss: {}\".format(step_loss))\n",
    "print(\"predictions \", end='')\n",
    "print(\"RMSE: {}\\n\".format(rmse_val))\n",
    "#print(\"prediction_last :{}\". format(prediction_last) )\n",
    "df1 = ((testY * ((test_max - test_min) + 1e-7) + test_min).astype(int))\n",
    "df1 = pd.DataFrame(df1)\n",
    "testY = df1[4]\n",
    "df2 = ((test_predict * ((test_max - test_min) + 1e-7) + test_min).astype(int))\n",
    "df2= pd.DataFrame(df2)\n",
    "pred = df2[4]\n",
    "df = pd.concat([pred,testY], axis=1)\n",
    "df.columns=['Pred','True']\n",
    "display(df)\n",
    "df['Pred'].plot(figsize=(16,4))\n",
    "df['True'].plot()\n",
    "plt.legend(loc=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Save - MinMaxScaler.fit "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "from tqdm import tqdm\n",
    "sns.set()\n",
    "tf.compat.v1.random.set_random_seed(1234)\n",
    "\n",
    "f =  open('d:\\\\/stock_prediction with LSTM_version-1.txt', 'a')\n",
    "print(\"===== Stock Prediction with LSTM_first =====\", file = f)\n",
    "print(\"start_day: {}\\n\".format(datetime.now()),file = f)\n",
    "\n",
    "tf.set_random_seed(777)  # reproducibility\n",
    "tf.reset_default_graph()\n",
    "\n",
    "test_min = np.min(xy, 0)\n",
    "test_max = np.max(xy, 0)\n",
    "test_denom = test_max - test_min\n",
    "\n",
    "engine = sqlalchemy.create_engine('mysql+pymysql://kkang:leaf2027@localhost/stock?charset=utf8',encoding='utf-8')\n",
    "    \n",
    "name = input('주식이름을 입력하세요 : ')\n",
    "date = input(\"날짜를 입력하세요 sample: '2019-01-10':\")\n",
    "        \n",
    "select_query = \"select * from market where Name= \"\n",
    "date_query = \"Date > \"\n",
    "\n",
    "var = select_query +\"'\"+name+\"'\"+\" \"+\"&&\"+\" \"+date_query+\"'\"+date+\"'\"\n",
    "\n",
    "df = pd.read_sql(var ,engine)\n",
    "#df = df[['Open','High','Low','Volume','Close']]\n",
    "df = df.iloc[:,3:]\n",
    "xy = df.values\n",
    "\n",
    "seq_length = 5\n",
    "data_dim = 5\n",
    "hidden_dim = 10\n",
    "output_dim = 1\n",
    "learning_rate = 0.01\n",
    "iterations = 9000\n",
    "\n",
    "minmax = MinMaxScaler().fit(df.astype('float64')) # Close index\n",
    "dataset = minmax.transform(df.astype('float64')) # Close index\n",
    "\n",
    "train_size = int(len(dataset) * 0.7)\n",
    "train_set = dataset[0:train_size]\n",
    "test_set = dataset[train_size-seq_length+1:]  # Index from [train_size - seq_leng\n",
    "\n",
    "# build datasets\n",
    "def build_dataset(time_series, seq_length):\n",
    "    dataX = []\n",
    "    dataY = []\n",
    "    for i in range(0, len(time_series) - seq_length):\n",
    "        _x = time_series[i:i + seq_length, :]\n",
    "        _y = time_series[i + seq_length, [-1]]  # Next close price\n",
    "        #print(_x, \"->\", _y)\n",
    "        dataX.append(_x)\n",
    "        dataY.append(_y)\n",
    "    return np.array(dataX), np.array(dataY)\n",
    "\n",
    "trainX, trainY = build_dataset(train_set, seq_length)\n",
    "#print(\"#\"*100)\n",
    "testX, testY = build_dataset(test_set, seq_length)\n",
    "#print(\"#\"*100)\n",
    "#lastX = last_set\n",
    "#print(lastX)\n",
    "\n",
    "# input place holders\n",
    "X = tf.placeholder(tf.float32, [None, seq_length, data_dim], name='first_input')\n",
    "Y = tf.placeholder(tf.float32, [None, 1], name=\"first_output\")\n",
    "\n",
    "# build a LSTM network\n",
    "cell = tf.nn.rnn_cell.LSTMCell(num_units=hidden_dim, state_is_tuple=True, activation=tf.tanh)\n",
    "outputs, _states = tf.nn.dynamic_rnn(cell, X, dtype=tf.float32)\n",
    "Y_pred = tf.contrib.layers.fully_connected(outputs[:, -1], output_dim, activation_fn=None)  # We use the last cell's output\n",
    "Y_pred = tf.identity(Y_pred, \"model\")\n",
    "# cost/loss\n",
    "loss = tf.reduce_sum(tf.square(Y_pred - Y))  # sum of the squares\n",
    "# optimizer\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "train = optimizer.minimize(loss)\n",
    "\n",
    "# RMSE\n",
    "targets = tf.placeholder(tf.float32, [None, 1])\n",
    "predictions = tf.placeholder(tf.float32, [None, 1])\n",
    "rmse = tf.sqrt(tf.reduce_mean(tf.square(targets - predictions)))\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "with tf.Session() as sess:\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "\n",
    "    # Training step\n",
    "    for i in range(iterations):\n",
    "        _, step_loss = sess.run([train, loss], feed_dict={X: trainX, Y: trainY})\n",
    "        if i % 100 == 0:\n",
    "            print(\"[step: {}] loss: {}\".format(i, step_loss))\n",
    "\n",
    "    # Test step\n",
    "    test_predict = sess.run(Y_pred, feed_dict={X: testX})\n",
    "    print(\"test_predict:{}\".format(test_predict))\n",
    "    rmse_val = sess.run(rmse, feed_dict={targets: testY, predictions: test_predict})\n",
    "    saver.save(sess, './first_model/first', global_step=1000)\n",
    "    \n",
    "    # Predictions test\n",
    "    #prediction_last = sess.run(Y_pred, feed_dict={X: lastX.reshape(1,seq_length,data_dim)})\n",
    "\n",
    "print(\"step_loss: {}\".format(step_loss),file = f)\n",
    "print(\"RMSE: {}\\n\".format(rmse_val),file = f)\n",
    "print(\"step_loss: {}\".format(step_loss))\n",
    "print(\"predictions \", end='')\n",
    "print(\"RMSE: {}\\n\".format(rmse_val))\n",
    "#print(\"prediction_last :{}\". format(prediction_last) )\n",
    "df1 = ((testY * ((test_max - test_min) + 1e-7) + test_min).astype(int))\n",
    "df1 = pd.DataFrame(df1)\n",
    "testY = df1[4]\n",
    "df2 = ((test_predict * ((test_max - test_min) + 1e-7) + test_min).astype(int))\n",
    "df2= pd.DataFrame(df2)\n",
    "pred = df2[4]\n",
    "df = pd.concat([pred,testY], axis=1)\n",
    "df.columns=['Pred','True']\n",
    "display(df)\n",
    "df['Pred'].plot(figsize=(16,4))\n",
    "df['True'].plot()\n",
    "plt.legend(loc=0)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Restore - MinMaxScaler.fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sqlalchemy\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "from tqdm import tqdm\n",
    "sns.set()\n",
    "\n",
    "tf.compat.v1.random.set_random_seed(1234)\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "engine = sqlalchemy.create_engine('mysql+pymysql://kkang:leaf2027@localhost/stock?charset=utf8',encoding='utf-8')\n",
    "    \n",
    "name = input('주식이름을 입력하세요 : ')\n",
    "date = input(\"날짜를 입력하세요 sample: '2019-01-10':\")\n",
    "        \n",
    "select_query = \"select * from market where Name= \"\n",
    "date_query = \"Date > \"\n",
    "\n",
    "var = select_query +\"'\"+name+\"'\"+\" \"+\"&&\"+\" \"+date_query+\"'\"+date+\"'\"\n",
    "\n",
    "df = pd.read_sql(var ,engine)\n",
    "#df = df[['Open','High','Low','Volume','Close']]\n",
    "df = df.iloc[:,3:]\n",
    "xy = df.values\n",
    "\n",
    "test_min = np.min(xy, 0)\n",
    "test_max = np.max(xy, 0)\n",
    "test_denom = test_max - test_min\n",
    "\n",
    "seq_length = 5\n",
    "data_dim = 5\n",
    "hidden_dim = 10\n",
    "output_dim = 1\n",
    "learning_rate = 0.01\n",
    "iterations = 900\n",
    "\n",
    "minmax = MinMaxScaler().fit(df.astype('float64')) # Close index\n",
    "dataset = minmax.transform(df.astype('float64')) # Close index\n",
    "\n",
    "train_size = int(len(dataset) * 0.7)\n",
    "train_set = dataset[0:train_size]\n",
    "test_set = dataset[train_size-seq_length+1:]  # Index from [train_size - seq_leng\n",
    "\n",
    "# build datasets\n",
    "def build_dataset(time_series, seq_length):\n",
    "    dataX = []\n",
    "    dataY = []\n",
    "    for i in range(0, len(time_series) - seq_length):\n",
    "        _x = time_series[i:i + seq_length, :]\n",
    "        _y = time_series[i + seq_length, [-1]]  # Next close price\n",
    "        #print(_x, \"->\", _y)\n",
    "        dataX.append(_x)\n",
    "        dataY.append(_y)\n",
    "    return np.array(dataX), np.array(dataY)\n",
    "\n",
    "trainX, trainY = build_dataset(train_set, seq_length)\n",
    "#print(\"#\"*100)\n",
    "testX, testY = build_dataset(test_set, seq_length)\n",
    "\n",
    "# input place holders\n",
    "#X = tf.placeholder(tf.float32, [None, seq_length, data_dim], name='first_input')  *** 중요 !!! restore에 name을 주면 안됨\n",
    "#Y = tf.placeholder(tf.float32, [None, 1], name=\"first_output\")\n",
    "X = tf.placeholder(tf.float32, [None, seq_length, data_dim])\n",
    "Y = tf.placeholder(tf.float32, [None, 1])\n",
    "\n",
    "sess = tf.Session()\n",
    "new_saver = tf.train.import_meta_graph('first_model/first-1000.meta')\n",
    "new_saver.restore(sess, tf.train.latest_checkpoint('./first_model'))\n",
    "\n",
    "X = sess.graph.get_tensor_by_name(\"first_input:0\")\n",
    "Y = sess.graph.get_tensor_by_name(\"first_output:0\")\n",
    "Y_pred = sess.graph.get_tensor_by_name(\"model:0\")\n",
    "\n",
    "test_predict = sess.run(Y_pred, feed_dict={X: testX})\n",
    "print(\"test_predict:{}\".format(test_predict))\n",
    "\n",
    "df1 = ((testY * ((test_max - test_min) + 1e-7) + test_min).astype(int))\n",
    "df1 = pd.DataFrame(df1)\n",
    "testY = df1[4]\n",
    "df2 = ((test_predict * ((test_max - test_min) + 1e-7) + test_min).astype(int))\n",
    "df2= pd.DataFrame(df2)\n",
    "pred = df2[4]\n",
    "df = pd.concat([pred,testY], axis=1)\n",
    "df.columns=['Pred','True']\n",
    "display(df)\n",
    "df['Pred'].plot(figsize=(16,4))\n",
    "df['True'].plot()\n",
    "plt.legend(loc=0)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class start:\n",
    "    def source(self):\n",
    "\n",
    "        import tensorflow as tf\n",
    "        import numpy as np\n",
    "        import matplotlib.pyplot as plt\n",
    "        import seaborn as sns\n",
    "        import sqlalchemy\n",
    "        import pandas as pd\n",
    "        from sklearn.preprocessing import MinMaxScaler\n",
    "        from datetime import datetime\n",
    "        from datetime import timedelta\n",
    "        from tqdm import tqdm\n",
    "        sns.set()\n",
    "\n",
    "        tf.compat.v1.random.set_random_seed(1234)\n",
    "\n",
    "        tf.reset_default_graph()\n",
    "\n",
    "        engine = sqlalchemy.create_engine('mysql+pymysql://kkang:leaf2027@localhost/stock?charset=utf8',encoding='utf-8')\n",
    "\n",
    "        name = input('주식이름을 입력하세요 : ')\n",
    "        date = input(\"날짜를 입력하세요 sample: '2019-01-10':\")\n",
    "\n",
    "        select_query = \"select * from market where Name= \"\n",
    "        date_query = \"Date > \"\n",
    "\n",
    "        var = select_query +\"'\"+name+\"'\"+\" \"+\"&&\"+\" \"+date_query+\"'\"+date+\"'\"\n",
    "\n",
    "        df = pd.read_sql(var ,engine)\n",
    "        #df = df[['Open','High','Low','Volume','Close']]\n",
    "        df = df.iloc[:,3:]\n",
    "        xy = df.values\n",
    "\n",
    "        test_min = np.min(xy, 0)\n",
    "        test_max = np.max(xy, 0)\n",
    "        test_denom = test_max - test_min\n",
    "\n",
    "        seq_length = 5\n",
    "        data_dim = 5\n",
    "        hidden_dim = 10\n",
    "        output_dim = 1\n",
    "        learning_rate = 0.01\n",
    "        iterations = 900\n",
    "\n",
    "        minmax = MinMaxScaler().fit(df.astype('float64')) # Close index\n",
    "        dataset = minmax.transform(df.astype('float64')) # Close index\n",
    "\n",
    "        train_size = int(len(dataset) * 0.7)\n",
    "        train_set = dataset[0:train_size]\n",
    "        test_set = dataset[train_size-seq_length+1:]  # Index from [train_size - seq_leng\n",
    "\n",
    "        # build datasets\n",
    "        def build_dataset(time_series, seq_length):\n",
    "            dataX = []\n",
    "            dataY = []\n",
    "            for i in range(0, len(time_series) - seq_length):\n",
    "                _x = time_series[i:i + seq_length, :]\n",
    "                _y = time_series[i + seq_length, [-1]]  # Next close price\n",
    "                #print(_x, \"->\", _y)\n",
    "                dataX.append(_x)\n",
    "                dataY.append(_y)\n",
    "            return np.array(dataX), np.array(dataY)\n",
    "\n",
    "        trainX, trainY = build_dataset(train_set, seq_length)\n",
    "        #print(\"#\"*100)\n",
    "        testX, testY = build_dataset(test_set, seq_length)\n",
    "\n",
    "        # input place holders\n",
    "        #X = tf.placeholder(tf.float32, [None, seq_length, data_dim], name='first_input')  *** 중요 !!! restore에 name을 주면 안됨\n",
    "        #Y = tf.placeholder(tf.float32, [None, 1], name=\"first_output\")\n",
    "        X = tf.placeholder(tf.float32, [None, seq_length, data_dim])\n",
    "        Y = tf.placeholder(tf.float32, [None, 1])\n",
    "\n",
    "        sess = tf.Session()\n",
    "        new_saver = tf.train.import_meta_graph('first_model/first-1000.meta')\n",
    "        new_saver.restore(sess, tf.train.latest_checkpoint('./first_model'))\n",
    "\n",
    "        X = sess.graph.get_tensor_by_name(\"first_input:0\")\n",
    "        Y = sess.graph.get_tensor_by_name(\"first_output:0\")\n",
    "        Y_pred = sess.graph.get_tensor_by_name(\"model:0\")\n",
    "\n",
    "        test_predict = sess.run(Y_pred, feed_dict={X: testX})\n",
    "        print(\"test_predict:{}\".format(test_predict))\n",
    "\n",
    "        df1 = ((testY * ((test_max - test_min) + 1e-7) + test_min).astype(int))\n",
    "        df1 = pd.DataFrame(df1)\n",
    "        testY = df1[4]\n",
    "        df2 = ((test_predict * ((test_max - test_min) + 1e-7) + test_min).astype(int))\n",
    "        df2= pd.DataFrame(df2)\n",
    "        pred = df2[4]\n",
    "        df = pd.concat([pred,testY], axis=1)\n",
    "        df.columns=['Pred','True']\n",
    "        display(df)\n",
    "        df['Pred'].plot(figsize=(16,4))\n",
    "        df['True'].plot()\n",
    "        plt.legend(loc=0)\n",
    "        plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "a = start()\n",
    "a.source()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
